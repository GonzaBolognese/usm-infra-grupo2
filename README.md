## üéØ Objetivos de las Actividades

### **Actividad 1 (Obligatoria)**: Agregar Columna Distribuidor
Agregar la columna `distribuidor` a todos los archivos CSV antes de cargarlos a BigQuery/Data Warehouse.

### **Actividad 2 (Opcional/Avanzada)**: Implementar Subida a Google Cloud Storage
Implementar el m√©todo `_upload_to_gcs()` para subir autom√°ticamente los archivos generados a un bucket de GCS.

---

## üìã ACTIVIDAD 1: Columna Distribuidor (OBLIGATORIA)

### ‚ùì ¬øPor qu√© agregar la columna distribuidor?

#### 1. **Particionado Eficiente en BigQuery**
```sql
-- Con la columna distribuidor puedes hacer:
SELECT * FROM ventas WHERE distribuidor = 1 AND fecha_cierre = '2024-01-01'

-- Sin la columna, tendr√≠as que hacer:
SELECT * FROM ventas WHERE _FILE_NAME LIKE '%Distribuidor_1%'
```

#### 2. **Mejores Consultas y Filtros**
- Filtrar por distribuidor es instant√°neo
- Agregaciones por distribuidor son m√°s eficientes  
- Joins entre tablas son m√°s simples

#### 3. **Particionado de Tablas**
```sql
-- BigQuery puede particionar autom√°ticamente:
CREATE TABLE dataset.ventas_partitioned
PARTITION BY distribuidor
AS SELECT * FROM ventas_raw
```

#### 4. **Control de Acceso**
- Diferentes equipos pueden acceder solo a su distribuidor
- Seguridad a nivel de fila basada en distribuidor
- Auditor√≠a m√°s granular

### üìç D√≥nde Agregar la Columna

Busca estos comentarios en `generador.py`:

#### 1. Archivo de Ventas (l√≠nea ~299)
```python
ventas.append({
    'sucursal': cliente.sucursal,
    'cliente': cliente.id_cliente,
    'fecha_cierre': fecha.strftime('%Y-%m-%d'),
    'sku': producto_sku,
    'venta_unidades': venta['cantidad'],
    'venta_importe': venta['importe'],
    'condicion_venta': cliente.condicion_venta
    # TODO ESTUDIANTES: Agregar columna 'distribuidor' aqu√≠
    # 'distribuidor': distribuidor
})
```

**‚úÖ Soluci√≥n:**
```python
ventas.append({
    'sucursal': cliente.sucursal,
    'cliente': cliente.id_cliente,
    'fecha_cierre': fecha.strftime('%Y-%m-%d'),
    'sku': producto_sku,
    'venta_unidades': venta['cantidad'],
    'venta_importe': venta['importe'],
    'condicion_venta': cliente.condicion_venta,
    'distribuidor': distribuidor  # ‚Üê AGREGAR ESTA L√çNEA
})
```

#### 2. Archivo de Stock (l√≠nea ~320)
```python
stock_record = {
    'sucursal': distribuidor * 100 + 1,
    'fecha_cierre': fecha_str,
    'sku': sku,
    'producto': PRODUCTOS[sku]['nombre'],
    'stock': info['cantidad'],
    'unidad': info['unidad']
    # TODO ESTUDIANTES: Agregar columna 'distribuidor' aqu√≠
    # 'distribuidor': distribuidor
}
```

**‚úÖ Soluci√≥n:**
```python
stock_record = {
    'sucursal': distribuidor * 100 + 1,
    'fecha_cierre': fecha_str,
    'sku': sku,
    'producto': PRODUCTOS[sku]['nombre'],
    'stock': info['cantidad'],
    'unidad': info['unidad'],
    'distribuidor': distribuidor  # ‚Üê AGREGAR ESTA L√çNEA
}
```

#### 3. Archivo Maestro (l√≠nea ~340)
```python
maestro_record = {
    'sucursal': cliente.sucursal,
    'cliente': cliente.id_cliente,
    # ... otros campos ...
    'tipo_negocio': cliente.tipo_negocio
    # TODO ESTUDIANTES: Agregar columna 'distribuidor' aqu√≠
    # 'distribuidor': distribuidor
}
```

**‚úÖ Soluci√≥n:**
```python
maestro_record = {
    'sucursal': cliente.sucursal,
    'cliente': cliente.id_cliente,
    # ... otros campos ...
    'tipo_negocio': cliente.tipo_negocio,
    'distribuidor': distribuidor  # ‚Üê AGREGAR ESTA L√çNEA
}
```

---

## ‚òÅÔ∏è ACTIVIDAD 2: Google Cloud Storage (OPCIONAL)

### üéØ Objetivo
Implementar el m√©todo `_upload_to_gcs()` para subir autom√°ticamente los archivos CSV generados a un bucket de Google Cloud Storage.

### üì¶ Preparaci√≥n

#### 1. Instalar Dependencias
```bash
pip install google-cloud-storage
```

#### 2. Configurar Credenciales
```bash
# Opci√≥n 1: Variable de entorno
export GOOGLE_APPLICATION_CREDENTIALS="path/to/credentials.json"

# Opci√≥n 2: Autenticaci√≥n con gcloud
gcloud auth application-default login
```

#### 3. Crear Bucket GCS
```bash
gsutil mb gs://mi-bucket-universidad
```

### üîß Implementaci√≥n

Busca el m√©todo `_upload_to_gcs()` en `generador.py` (l√≠nea ~XX) y reemplaza el contenido:

**üöÄ C√≥digo a Implementar:**
```python
def _upload_to_gcs(self, archivos, bucket_name, distribuidor):
    """
    Sube archivos a Google Cloud Storage.
    """
    try:
        from google.cloud import storage
        
        # Crear cliente GCS
        client = storage.Client()
        bucket = client.bucket(bucket_name)
        
        # Subir cada archivo
        for archivo in archivos:
            if os.path.exists(archivo):
                # Estructura: data/distribuidor_X/archivo.csv
                blob_name = f"data/distribuidor_{distribuidor}/{os.path.basename(archivo)}"
                blob = bucket.blob(blob_name)
                
                # Subir archivo
                blob.upload_from_filename(archivo)
                print(f"    ‚òÅÔ∏è Subido a GCS: {blob_name}")
                
    except ImportError:
        print("    ‚ö†Ô∏è google-cloud-storage no instalado")
    except Exception as e:
        print(f"    ‚ùå Error subiendo a GCS: {str(e)}")
```

### üîå Activar Subida GCS

Busca el comentario en el bucle principal (l√≠nea ~XX):

```python
# üéØ ACTIVIDAD ADICIONAL PARA ESTUDIANTES:
# Implementar subida a Google Cloud Storage aqu√≠
# Ejemplo de uso:
# archivos_a_subir = [archivo_ventas, archivo_stock]
# if dia == 0:  # Agregar maestro solo el primer d√≠a
#     archivos_a_subir.append(archivo_maestro)
# self._upload_to_gcs(archivos_a_subir, 'nombre-bucket', distribuidor)
```

**‚úÖ Soluci√≥n:**
```python
# Subir archivos a GCS (implementado por estudiantes)
archivos_a_subir = [archivo_ventas, archivo_stock]
if dia == 0:  # Agregar maestro solo el primer d√≠a
    archivos_a_subir.append(archivo_maestro)
self._upload_to_gcs(archivos_a_subir, 'mi-bucket-universidad', distribuidor)
```

### üèóÔ∏è Estructura Resultante en GCS
```
mi-bucket-universidad/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ distribuidor_1/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Venta_Clientes_2024-01-01.csv
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ StockPeriodo_2024-01-01.csv
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Maestro_2024-01-01.csv
‚îÇ   ‚îú‚îÄ‚îÄ distribuidor_2/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Venta_Clientes_2024-01-01.csv
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îî‚îÄ‚îÄ distribuidor_3/
‚îî‚îÄ‚îÄ ...
```

---

## üß™ Verificando las Implementaciones

### 1. Ejecutar el generador
```bash
python generador.py
```

### 2. Verificar Actividad 1: Columna Distribuidor
```bash
# Verificar headers de archivos generados
head -1 output/Archivos_VentaClientes/Distribuidor_1/Venta_Clientes_*.csv
head -1 output/Archivos_Stock/Distribuidor_1/StockPeriodo_*.csv  
head -1 output/Archivos_Maestro/Distribuidor_1/Maestro_*.csv
```

**‚úÖ Deber√≠as ver:**
```
sucursal,cliente,fecha_cierre,sku,venta_unidades,venta_importe,condicion_venta,distribuidor
sucursal,fecha_cierre,sku,producto,stock,unidad,distribuidor
sucursal,cliente,ciudad,provincia,estado,...,distribuidor
```

### 3. Verificar Actividad 2: Subida GCS
```bash
# Listar archivos en GCS
gsutil ls -r gs://mi-bucket-universidad/

# Deber√≠a mostrar:
# gs://mi-bucket-universidad/data/distribuidor_1/Venta_Clientes_2024-XX-XX.csv
# gs://mi-bucket-universidad/data/distribuidor_1/StockPeriodo_2024-XX-XX.csv
# etc.
```

---

## üìä Cargando a BigQuery

### 1. Desde GCS (Recomendado)
```sql
-- Crear tabla particionada por distribuidor
CREATE TABLE `proyecto.distribucion_data.ventas`
(
  sucursal INT64,
  cliente INT64,
  fecha_cierre DATE,
  sku STRING,
  venta_unidades INT64,
  venta_importe FLOAT64,
  condicion_venta STRING,
  distribuidor INT64
)
PARTITION BY distribuidor;

-- Cargar desde GCS
LOAD DATA INTO `proyecto.distribucion_data.ventas`
FROM FILES (
  format = 'CSV',
  skip_leading_rows = 1,
  uris = ['gs://mi-bucket-universidad/data/*/Venta_Clientes_*.csv']
);
```

### 2. Desde Local
```bash
# Cargar usando bq command line
bq load --source_format=CSV --skip_leading_rows=1 \
  proyecto:distribucion_data.ventas \
  output/Archivos_VentaClientes/*/*.csv
```

---

## üìà Consultas Optimizadas

### Ventas por Distribuidor
```sql
SELECT 
    distribuidor,
    COUNT(*) as total_ventas,
    SUM(venta_importe) as facturacion_total
FROM `proyecto.distribucion_data.ventas`
WHERE distribuidor = 1  -- Solo procesa partici√≥n 1
  AND fecha_cierre >= '2024-01-01'
GROUP BY distribuidor;
```

### Stock Cr√≠tico por Distribuidor
```sql
SELECT 
    distribuidor,
    sku,
    producto,
    stock
FROM `proyecto.distribucion_data.stock`
WHERE distribuidor = 2  -- Solo partici√≥n 2
  AND stock < 100
ORDER BY stock ASC;
```

### An√°lisis Cross-Distribuidor
```sql
-- Comparar performance entre distribuidores
SELECT 
    distribuidor,
    AVG(venta_importe) as ticket_promedio,
    COUNT(DISTINCT cliente) as clientes_unicos,
    SUM(venta_importe) as facturacion_total
FROM `proyecto.distribucion_data.ventas`
WHERE fecha_cierre >= '2024-01-01'
GROUP BY distribuidor
ORDER BY facturacion_total DESC;
```

---

## ‚úÖ Checklist de Completitud

### Actividad 1: Columna Distribuidor
- [ ] Agregu√© columna `distribuidor` en ventas
- [ ] Agregu√© columna `distribuidor` en stock  
- [ ] Agregu√© columna `distribuidor` en maestro
- [ ] Ejecut√© el generador sin errores
- [ ] Verifiqu√© que los CSV incluyan la nueva columna
- [ ] Los valores de distribuidor son correctos (1, 2, 3)

### Actividad 2: GCS Upload (Opcional)
- [ ] Instal√© google-cloud-storage
- [ ] Configur√© credenciales de GCS
- [ ] Cre√© bucket en GCS
- [ ] Implement√© m√©todo _upload_to_gcs()
- [ ] Activ√© llamada al m√©todo en el bucle principal
- [ ] Verifiqu√© que archivos se suban a GCS correctamente
- [ ] La estructura de carpetas en GCS es correcta

### BigQuery Integration
- [ ] Cargu√© al menos un archivo en BigQuery
- [ ] Cre√© tabla particionada por distribuidor
- [ ] Ejecut√© consultas con filtro por distribuidor
- [ ] Verifiqu√© mejora de performance vs tabla no particionada

---

## üèÜ Beneficios Obtenidos

1. **Performance**: Consultas 10x m√°s r√°pidas con particionado
2. **Costos**: BigQuery solo procesa particiones necesarias
3. **Escalabilidad**: Pipeline autom√°tico local ‚Üí GCS ‚Üí BigQuery
4. **Mantenibilidad**: C√≥digo SQL m√°s limpio y legible
5. **Profesional**: Experiencia con herramientas cloud reales

## ü§î Preguntas de Reflexi√≥n

1. **¬øQu√© ventajas tiene usar GCS como intermediario vs subir directo a BigQuery?**
2. **¬øC√≥mo implementar√≠as particionado por fecha adem√°s de distribuidor?**
3. **¬øQu√© estrategia usar√≠as para datos incremental vs full refresh?**
4. **¬øC√≥mo manejar√≠as errores en la subida a GCS en un entorno productivo?**

---

## üí° Conceptos Avanzados

### Particionado H√≠brido
```sql
CREATE TABLE ventas_optimizada
PARTITION BY distribuidor
CLUSTER BY fecha_cierre, cliente;
```

### Pipeline Completo
```bash
# 1. Generar datos
python generador.py

# 2. Subir a GCS (autom√°tico si implementaste)

# 3. Cargar a BigQuery
bq load --source_format=CSV --skip_leading_rows=1 \
  dataset.ventas gs://bucket/data/*/Venta_*.csv

# 4. Ejecutar an√°lisis
bq query "SELECT distribuidor, SUM(venta_importe) FROM dataset.ventas GROUP BY 1"
``` 
